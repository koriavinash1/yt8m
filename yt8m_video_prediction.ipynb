{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This network is to predict labels for given video\n",
    "\n",
    "+ Computationally very-very expensive\n",
    "+ Network uses vgg19 net to predict featres from each frame of video \n",
    "+ Network uses bidirectional LSTM to learn encode video feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named vgg19",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-48c7a7b0b1dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mvgg19\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named vgg19"
     ]
    }
   ],
   "source": [
    "# import all dependencies\n",
    "# TODO: add test and validation code\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
    "import numpy as np\n",
    "import math\n",
    "import vgg19\n",
    "import utils\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "import extract_video_info\n",
    "yt8m = extract_video_info.read_data_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all constants\n",
    "\n",
    "n_inputs_100 = 100\n",
    "n_inputs_200 = 200\n",
    "n_inputs_300 = 300\n",
    "\n",
    "n_classes = 4716\n",
    "learning_rate = 0.0001\n",
    "batch_size = 5\n",
    "dropout = 0.5 \n",
    "epochs = 500\n",
    "\n",
    "sub_graph_input = 4096 # depends on feature dimension extracted from vgg19\n",
    "\n",
    "display_steps = 1\n",
    "std_height = 224\n",
    "std_width = 224\n",
    "\n",
    "test_examples = 10\n",
    "validation_examples = 10\n",
    "\n",
    "device = \"/cpu:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# misc funcs\n",
    "def defineVariables(shape, name):\n",
    "    initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "    return tf.get_variable(name = name, shape = shape, initializer=initializer, dtype=tf.float32)\n",
    "\n",
    "def preActivation(x, w, b):\n",
    "    return tf.add(tf.matmul(x, w), b)\n",
    "\n",
    "def activation(x):\n",
    "    return tf.nn.relu(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE = 0.0001, BATCH SIZE = 5, EPOCHS = 500\n"
     ]
    }
   ],
   "source": [
    "# network status\n",
    "\n",
    "print \"LEARNING RATE = {}\".format(learning_rate)+\", BATCH SIZE = {}\".format(batch_size)+\", EPOCHS = {}\".format(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders time\n",
    "video_labels = tf.placeholder(tf.bool, shape=(None, n_classes))\n",
    "encoder_inputs = tf.placeholder(tf.float32, shape=(None, None, 4096))\n",
    "encoder_hidden_units = tf.placeholder(tf.float32)\n",
    "\n",
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# video preprocessing\n",
    "\n",
    "def preProcessing(videos):\n",
    "    feature_videos = []\n",
    "    print \"preprocesing videos\", len(videos)\n",
    "\n",
    "    with tf.device(device):\n",
    "        vgg = vgg19.Vgg19()\n",
    "        video_placeholder = tf.placeholder(tf.float32, [1, 224, 224, 3])\n",
    "        with tf.name_scope(\"content_vgg\"):\n",
    "            vgg.build(video_placeholder)\n",
    "        for video in videos:\n",
    "            feature_video = []\n",
    "            for frame in video:\n",
    "                input_subgraph = sess.run(vgg.fc6, feed_dict={video_placeholder: [frame], keep_prob: dropout})\n",
    "                feature_video.append(input_subgraph[0])\n",
    "            feature_videos.append(np.array(feature_video, ndmin=2))\n",
    "            print \"updates features in video\", len(feature_videos)\n",
    "        np.array(feature_videos, ndmin=3).shape\n",
    "    return np.array(feature_videos, ndmin=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define weights and biases\n",
    "\n",
    "weights = {\n",
    "    'wih1': defineVariables([4*300, 24000], \"wih1\"),\n",
    "    'wh1h2': defineVariables([24000, 24000], \"wh1h2\"),\n",
    "    'wh4o': tf.Variable(tf.truncated_normal([24000, n_classes]), \"wh4o\")\n",
    "    }\n",
    "\n",
    "biases = {\n",
    "    'bi': defineVariables([24000], \"bi\"),\n",
    "    'bh1' :defineVariables([24000], \"bh1\"),\n",
    "    'bh4': tf.Variable(tf.truncated_normal([n_classes]), \"bh4\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# main network\n",
    "\n",
    "def main_network(encoder_inputs_embedded, encoder_hidden_units):\n",
    "\twith tf.device(device):\n",
    "\t\t# size : number of frames = encoder hidden state units\n",
    "\t\t# feature embedding....\t\n",
    "\t\t# embeddings = tf.Variable(tf.random_uniform([size, encoder_hidden_units], -1.0, 1.0), dtype=tf.float32, name=\"embedding\")\n",
    "\t\t# encoder_inputs_embedded = tf.nn.embedding_lookup(embeddings, encoder_inputs, name = \"embedded\")\n",
    "\n",
    "\t\t# data encoding...\n",
    "\t\tfw_encoder_cell = LSTMCell(300)\n",
    "\t\tbw_encoder_cell = LSTMCell(300)\n",
    "\t\tprint encoder_inputs_embedded.get_shape()\n",
    "        \n",
    "        # bidirectional rnn without attention\n",
    "\t\t(encoder_fw_outputs, encoder_bw_outputs), (encoder_fw_final_state, encoder_bw_final_state) = tf.nn.bidirectional_dynamic_rnn(cell_fw=fw_encoder_cell, cell_bw=bw_encoder_cell, inputs=encoder_inputs_embedded, dtype=tf.float32)\n",
    "\n",
    "\t\tencoder_final_state_c = tf.concat((encoder_fw_final_state.c, encoder_bw_final_state.c), 1)\n",
    "\t\tencoder_final_state_h = tf.concat((encoder_fw_final_state.h, encoder_bw_final_state.h), 1)\n",
    "\t\tencoder_final_state = tf.concat((encoder_final_state_c, encoder_final_state_h), 1)\n",
    "\n",
    "\t\t# MLP for further classification task...\n",
    "\t   \twith tf.variable_scope('main_net_layer1') as scope:\n",
    "\t\t\t# tf.summary.histogram(\"weights\", weights['wih1'])\n",
    "\t\t\t# tf.summary.histogram(\"biases\", biases['bi'])\n",
    "\t\t\tfc1 = preActivation(encoder_final_state, weights['wih1'], biases['bi'])\n",
    "\t\t\tfc1_out = activation(fc1)\n",
    "\t\t\tfc1_dropped = tf.nn.dropout(fc1_out, keep_prob, name=\"dropout\")\n",
    "\n",
    "\t\twith tf.variable_scope('main_net_layer2') as scope:\n",
    "\t\t\t# tf.summary.histogram(\"weights\", weights['wh1h2'])\n",
    "\t\t\t# tf.summary.histogram(\"biases\", biases['bh1'])\n",
    "\t\t\tfc2 = preActivation(fc1_dropped, weights['wh1h2'], biases['bh1'])\n",
    "\t\t\tfc2_out = activation(fc2)\n",
    "\t\t\tfc2_dropped = tf.nn.dropout(fc2_out, keep_prob, name=\"dropout\")\n",
    "\n",
    "\t\twith tf.variable_scope('main_net_layer3') as scope:\n",
    "\t\t\t# tf.summary.histogram(\"weights\", weights['wh4o'])\n",
    "\t\t\t# tf.summary.histogram(\"biases\", biases['bh4'])\n",
    "\t\t\tout = preActivation(fc2_dropped, weights['wh4o'], biases['bh4'])\n",
    "\t\t\t# out = activation(out)\n",
    "\treturn out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction and optimizer\n",
    "\n",
    "pred = main_network(encoder_inputs, encoder_hidden_units)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=video_labels))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "tf.summary.scalar(\"cost\", cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# accuracy calculation\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(tf.cast(video_labels, dtype=tf.float32), 1))\n",
    "\n",
    "# correct_pred = tf.where(tf.less(tf.subtract(tf.nn.softmax(pred), tf.cast(video_labels, dtype=tf.float32)), 0.15), tf.nn.softmax(pred), tf.zeros_like(video_labels))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "tf.summary.scalar(\"acc\", accuracy)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# run code\n",
    "\n",
    "with tf.device(device):\n",
    "\twith tf.Session() as sess:\n",
    "\t\tstart_time = time.time()\n",
    "\t\tsess.run(init)\n",
    "\t\tstep = 1\n",
    "\t\ttrain_writer = tf.summary.FileWriter('./logs', sess.graph)\n",
    "\t\twhile step <= epochs * 30 / batch_size:\n",
    "\t\t# batch_100 an array of video frames, video ids, video labels \n",
    "\t\t\tprint str(time.time() - start_time) + \"sec\"\n",
    "\t\t\tbatch_100, batch_200, batch_300 = yt8m.train.next_batch(batch_size)\n",
    "\t\t\tenc_inputs = preProcessing(batch_300[0])\n",
    "\t\t\tprint enc_inputs.shape\n",
    "\t\t\tsess.run(optimizer, feed_dict={video_labels: np.array(batch_200[2].tolist(), ndmin=2), encoder_inputs:enc_inputs, encoder_hidden_units: 300, keep_prob:0.5})\n",
    "\n",
    "\n",
    "\t\t\tif step % display_steps == 0:\n",
    "\t\t\t\tsummary, loss = sess.run([merged, cost], feed_dict={video_labels: np.array(batch_300[2].tolist(), ndmin=2),  encoder_inputs:enc_inputs, encoder_hidden_units: 300, keep_prob:0.5})\n",
    "\t\t\t\ttrain_writer.add_summary(summary, step)\n",
    "\t\t\t\tprint \"loss: {}\".format(loss)\n",
    "\n",
    "\t        step += 1\n",
    "\n",
    "\t\tprint \"Optimization Finished!\"\n",
    "\t\tprint \"training time = {}\".format(time.time() - start_time)\n",
    "\tpass\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to implement multiple bucket handling code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
